# -*- coding: utf-8 -*-
"""new Ml

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fj-9YduG1TOpGLQuvYqNMgzy7i4d3I97
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

"""# **Data Explortion and Analysis**

Data analysis is the process of examining, cleaning, transforming, and interpreting data to discover meaningful insights, trends, and patterns. It involves a variety of techniques and methods to extract valuable information from raw data, with the ultimate goal of making informed decisions or drawing conclusions.
"""

import pandas as pd

# Load the data into a DataFrame
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')

from google.colab import drive
drive.mount('/content/drive')

df.head(20)

"""# **Data Information**
Number of column= 12
Number of entries=5110

Features are 'id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married',
       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',
       'smoking_status', 'stroke'],
      dtype='object'
"""

df.info()

"""# **Data types**
Data type refers to the data contained in each column.

The "id" column contains integer values

The "gender", "ever_married", "work_type", "Residence_type", and "smoking_status" columns contain string values (object).

The "age", "avg_glucose_level", and "bmi" columns contain floating-point values

The "hypertension", "heart_disease", and "stroke" columns contain integer values (int64).
"""

print(df.dtypes)

"""# **Data cleaning**
Check for missing values and check for duplications and dropping of a column=id

201 values were missing=BMI and there were no duplicates

id column is being dropped giving us a new total of 11 colums instead of 12
"""

missing_values = df.isna().sum()
print(missing_values)
duplicate_rows = df[df.duplicated()]

print(duplicate_rows)

import pandas as pd

# Assuming df is your DataFrame and 'column_to_drop' is the name of the column you want to drop
column_to_drop = 'column_name'

# Drop the specified column
df.drop(columns=['id'], inplace=True)

# Print the DataFrame to see the remaining columns
print(df)

"""# **This is to show id being taken off**"""

df.info()

df.head(5)

"""# **Dropping all missing values=BMI**

The toatal missing values were 201=BMI. After dropping the BMI columns educe to 4909, which means 201 patients has unrecorded BMI
"""

# Drop rows with missing values
df_cleaned = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned)

df.info()

"""# **Visualization**

to make age grouping eassy for me, i decided to haave an age bin of 10 years intvals

"""

import pandas as pd
# Read the dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')

# Gender distribution
gender_distribution = df['gender'].value_counts()

# Age distribution
# Define age groups
age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
age_labels = ['1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']
# Create age groups
df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)
# Stroke distribution by age group
stroke_by_age_distribution = df.groupby(['age_group', 'stroke']).size()


# Stroke distribution by gender
stroke_by_gender_distribution = df.groupby(['gender', 'stroke']).size()

# Hypertension distribution
hypertension_distribution = df['hypertension'].value_counts()

# Heart disease distribution
heart_disease_distribution = df['heart_disease'].value_counts()

print("Gender Distribution:")
print(gender_distribution)
print("Stroke Distribution by Age Group:")
print(stroke_by_age_distribution)
print("\nStroke Distribution by Gender:")
print(stroke_by_gender_distribution)
print("\nHypertension Distribution:")
print(hypertension_distribution)
print("\nHeart Disease Distribution:")
print(heart_disease_distribution)

import matplotlib.pyplot as plt

# Plot Gender Distribution
gender_distribution.plot(kind='bar', color=['blue', 'pink'])
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Plot Stroke Distribution by Age Group
stroke_by_age_distribution.unstack().plot(kind='bar')
plt.title('Stroke Distribution by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])
plt.show()

# Plot Stroke Distribution by Gender
stroke_by_gender_distribution.unstack().plot(kind='bar', color=['blue', 'orange'])
plt.title('Stroke Distribution by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])
plt.show()

# Plot Hypertension Distribution
plt.pie(hypertension_distribution, labels=hypertension_distribution.index, autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])
plt.title('Hypertension Distribution')
plt.show()

# Plot Heart Disease Distribution
plt.pie(heart_disease_distribution, labels=heart_disease_distribution.index, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])
plt.title('Heart Disease Distribution')
plt.show()

# Plot Stroke Distribution by Gender
stroke_by_gender_distribution.unstack().plot(kind='bar', color=['blue', 'orange'])
plt.title('Stroke Distribution by Smoking_status')
plt.xlabel('Smoking_status')
plt.ylabel('Count')
plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])
plt.show()

df.info()

missing_values = df.isna().sum()
print(missing_values)

df_cleaned = df.dropna()

# Print the cleaned DataFrame
print(df_cleaned)

df.info()

df_0 = df[df.iloc[:,-1]==0]
df_1 = df[df.iloc[:,-1]==1]

df['stroke'].value_counts()

df = pd.DataFrame(df)
df.columns = ['id','gender', 'age', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'avg_glucose_level', 'bmi','smoking_status', 'stroke','age_group']

# visualize balanced data
stroke = dict(df['stroke'].value_counts())
fig = px.pie(names = ['False','True'],values = stroke.values(),title = 'Stroke Occurance')
fig.update_traces(textposition='inside', textinfo='percent+label')

# Importing necessary libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the DecisionTreeClassifier
clf = DecisionTreeClassifier()

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Read the dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')

# Drop rows with missing values for simplicity
df.dropna(inplace=True)

# Split features and target variable
X = df.drop(columns=['stroke'])
y = df['stroke']

# Define categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Define preprocessing pipeline for categorical features
categorical_pipeline = Pipeline([
    ('onehot', OneHotEncoder())
])

# Define preprocessing pipeline for numerical features (no transformation needed)
numerical_pipeline = Pipeline([
    ('noop', 'passthrough')
])

# Combine preprocessing pipelines
preprocessing = ColumnTransformer([
    ('categorical', categorical_pipeline, categorical_features),
    ('numerical', numerical_pipeline, numerical_features)
])

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Define pipeline with preprocessing and classifier
pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('classifier', rf_classifier)
])

# Define hyperparameters for grid search
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best model from grid search
best_rf_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_rf_model.predict(X_test)

# Print classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame before cleaning
# Create pair plot before cleaning
sns.pairplot(df)
plt.title('Pair Plot - Before Cleaning')
plt.show()

# Assuming df_cleaned is your DataFrame after cleaning
# Create pair plot after cleaning
sns.pairplot(df_cleaned)
plt.title('Pair Plot - After Cleaning')
plt.show()

df.info()

import pandas as pd

# Assuming df is your DataFrame
# Check for missing values
missing_values = df.isnull().sum()

# Print the number of missing values for each column
print("Missing Values:")
print(missing_values)

df.describe()

df.info()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'column_name' is the name of the column you want to visualize
plt.figure(figsize=(8, 6))
sns.boxplot(x='age', data=df)
plt.title('Box Plot of ' + 'age')

plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'column_name' is the name of the column you want to visualize
plt.figure(figsize=(8, 6))
sns.boxplot(x='bmi', data=df)
plt.title('Box Plot of ' + 'bmi')
plt.show()

# Assuming df is your DataFrame and 'column_name' is the name of the column you want to visualize
plt.figure(figsize=(8, 6))
sns.boxplot(x='avg_glucose_level', data=df)
plt.title('Box Plot of ' + 'avg_glucose_level')
plt.show()

"""## *Imbalancce *
Taking a look at the diagram it can be clearly seen that the number of people recorded t have stroke is huge than those without stroke. same applies to hypertension and heart diesase.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'target_column' is the name of the target variable
plt.figure(figsize=(6, 4))
sns.countplot(x='heart_disease', data=df, palette='Set2')  # Assuming binary classes (Set2 palette)
plt.title('Class Distribution')
plt.xlabel('heart_disease')
plt.ylabel('Count')
plt.show(5)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'target_column' is the name of the target variable
plt.figure(figsize=(6, 4))
sns.countplot(x='stroke', data=df, palette='Set2')  # Assuming binary classes (Set2 palette)
plt.title('Class Distribution')
plt.xlabel('stroke')
plt.ylabel('Count')
plt.show(5)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'target_column' is the name of the target variable
plt.figure(figsize=(6, 4))
sns.countplot(x='hypertension', data=df, palette='Set2')  # Assuming binary classes (Set2 palette)
plt.title('Class Distribution')
plt.xlabel('hypertension')
plt.ylabel('Count')
plt.show(5)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'target_column' is the name of the target variable
plt.figure(figsize=(6, 4))
sns.countplot(x='avg_glucose_level', data=df, palette='Set2')  # Assuming binary classes (Set2 palette)
plt.title('Class Distribution')
plt.xlabel('avg_glucose_level')
plt.ylabel('Count')
plt.show(5)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame with 'stroke' and 'Residence_type' columns
plt.figure(figsize=(8, 6))
sns.countplot(x='stroke', hue='Residence_type', data=df, palette='Set2')
plt.title('Stroke Distribution by Residence Type')
plt.xlabel('Stroke Occurrence')
plt.ylabel('Count')
plt.legend(title='Residence Type')
plt.show()

# Assuming df is your DataFrame
ever_married_counts = df['ever_married'].value_counts()

# Print the count of people who are ever married
if 'Yes' in ever_married_counts:
    print("Number of people ever married:", ever_married_counts['Yes'])
else:
    print("Number of people ever married: 0")

# Assuming df is your DataFrame
ever_married_counts = df['ever_married'].value_counts()

# Print the count of people who are ever married
if 'Yes' in ever_married_counts:
    print("Number of people ever married:", ever_married_counts['No'])
else:
    print("Number of people ever married: 0")
plt.show()

# Assuming df is your DataFrame and 'avg_glucose_level' is the column containing glucose levels
min_glucose_level = df['avg_glucose_level'].min()
max_glucose_level = df['avg_glucose_level'].max()

# Calculate the range
glucose_level_range = max_glucose_level - min_glucose_level

print("Minimum Glucose Level:", min_glucose_level)
print("Maximum Glucose Level:", max_glucose_level)
print("Glucose Level Range:", glucose_level_range)

import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'avg_glucose_level' and 'stroke' are the columns containing glucose levels and stroke information, respectively
min_glucose_level = df['avg_glucose_level'].min()

# Filter the DataFrame for people with the minimum glucose level who had a stroke
min_glucose_stroke_df = df[(df['avg_glucose_level'] == min_glucose_level) & (df['stroke'] == 1)]

# Count the number of people with stroke
stroke_count = min_glucose_stroke_df.shape[1]

# Plot the result in a bar plot
plt.figure(figsize=(6, 4))
plt.bar(['Minimum Glucose Level'], [stroke_count], color='skyblue')
plt.title('Number of People with Minimum Glucose Level who had Stroke')
plt.xlabel('Glucose Level')
plt.ylabel('Stroke')
plt.show()

# Assuming df is your DataFrame and 'avg_glucose_level' and 'stroke' are the columns containing glucose levels and stroke information, respectively
max_glucose_level = df['avg_glucose_level'].max()

# Filter the DataFrame for people with the maximum glucose level who had a stroke
max_glucose_stroke_df = df[(df['avg_glucose_level'] == max_glucose_level) & (df['stroke'] == 1)]

# Count the number of people with stroke
stroke_count = max_glucose_stroke_df.shape[1]

# Plot the result in a bar plot
plt.figure(figsize=(6, 4))
plt.bar(['Maximum Glucose Level'], [stroke_count], color='skyblue')
plt.title('Number of People with Mamimum Glucose Level who had Stroke')
plt.xlabel('Glucose Level')
plt.ylabel('Stroke')
plt.show()

"""# **Class distribution after undersampling **"""

from imblearn.under_sampling import RandomUnderSampler

# Assuming X and y are your feature matrix and target vector, respectively
# Assuming your target variable is binary, with 0 representing the majority class and 1 representing the minority class

# Instantiate RandomUnderSampler
undersampler = RandomUnderSampler(random_state=42)

# Perform undersampling
X_undersampled, y_undersampled = undersampler.fit_resample(X, y)

# Check the class distribution after undersampling (optional)
print("Class Distribution after Undersampling:")
print(pd.Series(y_undersampled).value_counts())
plt.show()



import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Read the dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')

# Drop rows with missing values for simplicity
df.dropna(inplace=True)

# Split features and target variable
X = df.drop(columns=['stroke'])
y = df['stroke']

# Define categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Define preprocessing pipeline for categorical features
categorical_pipeline = Pipeline([
    ('onehot', OneHotEncoder())
])

# Define preprocessing pipeline for numerical features (no transformation needed)
numerical_pipeline = Pipeline([
    ('noop', 'passthrough')
])

# Combine preprocessing pipelines
preprocessing = ColumnTransformer([
    ('categorical', categorical_pipeline, categorical_features),
    ('numerical', numerical_pipeline, numerical_features)
])

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Define pipeline with preprocessing and classifier
pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('classifier', rf_classifier)
])

# Define hyperparameters for grid search
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best model from grid search
best_rf_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_rf_model.predict(X_test)

# Print classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

from sklearn.preprocessing import LabelEncoder

# Example data
gender = ['Female']

# Initialize LabelEncoder
encoder = LabelEncoder()

# Fit and transform the data
encoded_gender = encoder.fit_transform(gender)

print(encoded_gender)

df.info()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.under_sampling import RandomUnderSampler

# Read the dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')

# Drop rows with missing values for simplicity
df.dropna(inplace=True)

# Split features and target variable
X = df.drop(columns=['stroke'])
y = df['stroke']

# Define categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Define preprocessing pipeline for categorical features
categorical_pipeline = Pipeline([
    ('onehot', OneHotEncoder())
])

# Define preprocessing pipeline for numerical features (no transformation needed)
numerical_pipeline = Pipeline([
    ('noop', 'passthrough')
])

# Combine preprocessing pipelines
preprocessing = ColumnTransformer([
    ('categorical', categorical_pipeline, categorical_features),
    ('numerical', numerical_pipeline, numerical_features)
])

# Instantiate RandomUnderSampler
undersampler = RandomUnderSampler(random_state=42)

# Perform undersampling
X_undersampled, y_undersampled = undersampler.fit_resample(X, y)

# Split the undersampled data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_undersampled, y_undersampled, test_size=0.2, random_state=42)

# Define the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Define pipeline with preprocessing and classifier
pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('classifier', rf_classifier)
])

# Define hyperparameters for grid search
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best model from grid search
best_rf_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_rf_model.predict(X_test)

# Print classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

from imblearn.over_sampling import SMOTE

# Assuming X and y are your feature matrix and target vector, respectively
# Initialize SMOTE
smote = SMOTE(random_state=42)

# Perform oversampling
'stroke_resampled','stroke_resampled' = smote.fit_resample(X, y)

# Assuming df is your DataFrame and 'target_variable' is the name of your target variable

# Separate features (X) and target variable (y)
X = df.drop(columns=['stroke'])
y = df['stroke']

# Now you can use X and y in the fit_resample() method
X_resampled, y_resampled = smote.fit_resample(X, y)