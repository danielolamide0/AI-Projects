{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaDeS_43hWGZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# The spacy English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Loading data\n",
        "data = pd.read_csv('clearterms_data.csv')\n",
        "\n",
        "# Converting to lowercase\n",
        "data['statement1'] = data['statement1'].str.lower()\n",
        "data['statement2'] = data['statement2'].str.lower()\n",
        "\n",
        "# Removing punctuation\n",
        "pattern = '[^\\w\\s]'\n",
        "data['statement1'] = data['statement1'].apply(lambda x: re.sub(pattern, '', str(x)))\n",
        "data['statement2'] = data['statement2'].apply(lambda x: re.sub(pattern, '', str(x)))\n",
        "\n",
        "# Setting stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Defining the lemmatization function\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "# Applying lemmatization to the DataFrame columns\n",
        "data['statement1'] = data['statement1'].apply(lemmatize_text)\n",
        "data['statement2'] = data['statement2'].apply(lemmatize_text)\n",
        "\n",
        "# Sorting the DataFrame based on the importance rank\n",
        "data = data.sort_values(by='importance_rank', ascending=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# To determine the number of statements that constitute the top 10%\n",
        "top_10_percent_threshold = int(len(data) * 0.10)\n",
        "\n",
        "# Labelling the top 10% as 'important' and the rest as 'unimportant'\n",
        "data['label'] = ['important' if idx < top_10_percent_threshold else 'unimportant' for idx in range(len(data))]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3q2dq803-4OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Concatenating texts columns\n",
        "data['combined_text'] = data['statement1'] + \" \" + data['statement2']\n",
        "\n",
        "# Creating TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# To Fit and transform the combined text data\n",
        "tfidf_matrix = vectorizer.fit_transform(data['combined_text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "ajpe92siQZDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Concatenating both texts columns for each row and tokenizing\n",
        "tokenized_data = [word_tokenize(text1 + \" \" + text2) for text1, text2 in zip(data['statement1'], data['statement2'])]\n",
        "\n",
        "# Training a Word2Vec model using the tokenized data\n",
        "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "#saved model\n",
        "word2vec_model.save(\"word2vec_model.model\")\n"
      ],
      "metadata": {
        "id": "_x06pVT2DYez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['combined_text'])\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing and training Word2Vec\n",
        "tokenized_data = [word_tokenize(text) for text in data['combined_text']]\n",
        "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initializing an array to store the combined vectors\n",
        "combined_vectors = np.zeros((len(tokenized_data), 100))  # Assuming 100-dimensional Word2Vec vectors\n",
        "\n",
        "for i, tokens in enumerate(tokenized_data):\n",
        "    doc_vector = np.zeros(100)\n",
        "    for token in tokens:\n",
        "        # To check if token is in both Word2Vec and TF-IDF vocabularies\n",
        "        if token in word2vec_model.wv.key_to_index and token in vectorizer.vocabulary_:\n",
        "            word_vector = word2vec_model.wv[token]\n",
        "            tfidf_weight = tfidf_matrix[i, vectorizer.vocabulary_[token]]\n",
        "            doc_vector += word_vector * tfidf_weight\n",
        "    if np.linalg.norm(doc_vector) != 0:\n",
        "        doc_vector /= np.linalg.norm(doc_vector)\n",
        "    combined_vectors[i] = doc_vector\n"
      ],
      "metadata": {
        "id": "GlUbkJYPDvpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data['label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(combined_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "model = svm.SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "6nKnsVOVVQiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, 'svm_model.pkl')\n"
      ],
      "metadata": {
        "id": "k4fWj77lXs6l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}